# MCP-Based Document Intelligence Platform

A production-grade **Model Context Protocol (MCP)** powered document intelligence system that enables **tool-aware, prompt-driven, retrieval-augmented question answering** over PDFs using **OpenAI**, **ChromaDB**, and a **custom MCP clientâ€“server architecture**.

This project demonstrates how modern LLM applications can expose **capabilities (tools, resources, prompts)** via MCP and allow intelligent clients to reason, retrieve, and respond dynamically.
---
## ğŸš€ What This Project Does

- ğŸ“„ **Indexes PDF documents** into a persistent Chroma vector store
- ğŸ§  **Exposes document search as an MCP tool** (`query_document`)
- ğŸ” **Performs semantic retrieval** using OpenAI embeddings
- ğŸ¤– **Lets an LLM decide when to call tools vs answer directly**
- ğŸ§© **Supports MCP resources** (readable PDFs)
- ğŸ§  **Supports MCP prompt templates** (deep analysis, extraction, etc.)
- ğŸ’¬ **Maintains multi-turn conversational memory**
- ğŸ” **Implements a full OpenAI tool-calling loop**
- ğŸ–¥ **Runs fully locally** via STDIO-based MCP transport

---
## ğŸ’¡ Why This Matters

Traditional RAG systems tightly couple retrieval logic with the application.

This project demonstrates a **protocol-first architecture** where:
- Capabilities are **discoverable**
- Clients are **model-agnostic**
- Tools, resources, and prompts are **first-class primitives**
- LLMs can **reason over what the system can do**

This mirrors how **enterprise agent platforms** and **multi-agent systems** are being built today.

---

## ğŸ—ï¸ High Level Architecture Diagram
  
```mermaid
flowchart LR
    User["ğŸ‘¤ User<br/>(Terminal)"]

    subgraph Client["ğŸ§  MCP Client"]
        CLI["client.py<br/>Chat Loop"]
        Memory["Conversation Memory<br/>(message_history)"]
        ToolLoop["OpenAI Tool Loop<br/>(function calling)"]
    end

    subgraph Server["ğŸ§© MCP Server"]
        MCP["MCP Server<br/>(stdio)"]
        Tools["Tools<br/>query_document"]
        Resources["Resources<br/>PDFs"]
        Prompts["Prompt Templates"]
    end

    subgraph Vector["ğŸ“¦ Vector Store"]
        Chroma["ChromaDB<br/>(Persistent)"]
        Emb["OpenAI Embeddings"]
    end

    User --> CLI
    CLI --> ToolLoop
    ToolLoop --> MCP
    MCP --> Tools
    MCP --> Resources
    MCP --> Prompts
    Tools --> Chroma
    Chroma --> Emb
```
---
## ğŸ—ï¸ Execution Sequence (End-to-End)
```mermaid
sequenceDiagram
    participant U as User
    participant C as MCP Client
    participant L as OpenAI LLM
    participant S as MCP Server
    participant V as ChromaDB

    U->>C: Ask a question
    C->>L: Send conversation + available tools
    L-->>C: Tool call decision (or direct answer)
    C->>S: Execute MCP tool (query_document)
    S->>V: Semantic search
    V-->>S: Top-K chunks
    S-->>C: Tool response
    C->>L: Send tool result
    L-->>C: Final grounded answer
    C-->>U: Display answer
```
---

## ğŸ“ Project Structure
```text
mcp-document-intelligence/
â”œâ”€â”€ MCP_Setup.ipynb           # One-time ingestion: PDF â†’ chunks â†’ embeddings â†’ Chroma
â”œâ”€â”€ mcp_server.py             # MCP server exposing tools, resources, prompts
â”œâ”€â”€ client.py                 # MCP client with OpenAI tool loop + chat UI
â”‚
â”œâ”€â”€ testing/
â”‚   â””â”€â”€ .gitkeep              # Placeholder (PDFs ignored by git)
â”œâ”€â”€ .gitignore                # Ignores envs, chroma, PDFs, caches
â”‚
â”œâ”€â”€ pyproject.toml            # uv project config
â”œâ”€â”€ uv.lock                   # Locked dependencies
â””â”€â”€ README.md                 # Project documentation
```
---
## ğŸ”„ End-to-End Pipeline

### 1ï¸âƒ£ Document Ingestion (Offline)

**Trigger**

Triggered manually via notebook

**Steps**
1. **Load PDF documents**
2. **Chunk documents into semantic segments**
3. **Generate embeddings using OpenAI**
4. **Persist vectors + metadata to ChromaDB**

- This step is decoupled from runtime querying.

---

### 2ï¸âƒ£ MCP Server Initialization

When the server starts:
- Registers **tools, resources, and prompts**.
- Connects to the **persistent Chroma collection**.
- Exposes everything via **MCP descriptors**.
- Clients can discover capabilities dynamically.

---

### 3ï¸âƒ£ Runtime Querying (Online)

1. **User asks a question**
2. **Client builds conversation history**
3. **Client sends:**
   - Messages
   - Available Tools
4. **OpenAI decides:**
   - Answer directly or
   - Call query_document
5. **Tool executes via MCP**
6. **Results returned to LLM**
7. **Final grounded answer generated**
---

### âœ… Tool-Aware Reasoning Example

- Direct Answer (No Tool Call)

```bash
  Query: What is the capital of Telangana?

â†’ LLM answers directly
  ```

- Tool-Based Answer
```bash
Query: What is the main topic discussed in the document?

â†’ LLM requests query_document
â†’ MCP executes semantic search
â†’ LLM grounds answer in retrieved chunks
```
---
## ğŸ› ï¸ Prerequisites

### Local Development
- **Python 3.11+**
- **[`uv`](https://github.com/astral-sh/uv)** â€“ fast Python package & environment manager
- **Git**
- **OpenAI, OpenAI API key**

---
## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/mcp-document-intelligence.git
cd mcp-document-intelligence
```

### 2ï¸âƒ£ Create and activate a virtual environment
This project uses uv for fast and reproducible Python environments.
```bash
uv venv
source .venv/bin/activate
```
You should now see (.venv) in your terminal prompt.

### 3ï¸âƒ£ Install dependencies
Install all required dependencies exactly as defined in pyproject.toml and uv.lock.
```bash
uv sync
```
### 4ï¸âƒ£ Configure environment variables
Create a .env file inside the weather/ directory:
```bash
OPENAI_API_KEY=your_openai_api_key
```
### 5ï¸âƒ£ Run ingestion (one-time)

```bash
uv run jupyter notebook MCP_Setup.ipynb
```

### Start MCP server and Client

```bash
uv run python client.py mcp_server.py
```

### Example query:


```bash
/prompts
/prompt deep_analysis methodology
/resources
/resource document://pdf/ft_guide
/tools
```
------

## ğŸš€ Future Enhancements
1. **Multi-round tool execution loop**
2. **Streaming responses**
3. **Authenticated MCP endpoints**
4. **Web-based client (FastAPI / WebSockets)**
5. **Multi-agent orchestration**

